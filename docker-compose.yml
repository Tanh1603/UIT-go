services:
  # Redis - Caching and geospatial indexing (THE BOTTLENECK)
  # ‚ö†Ô∏è KEEP CPU LOW to ensure Redis is the bottleneck for driver matching algorithm tests
  # ‚úÖ Connected to app-monitor-net for observability stack access
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379" # ‚úÖ Exposed to host for seed-ghost-drivers.js access
    networks:
      - uit-go-network # ‚úÖ Internal app services
      - app-monitor-net # ‚úÖ Observability stack (Prometheus, Grafana, load tests)
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: "0.5" # ‚ö†Ô∏è Keep low - this is the bottleneck we want to test
          memory: 256M # ‚úÖ Reduced from 512M (100k keys = ~50MB, 256M is plenty)
        reservations:
          cpus: "0.25"
          memory: 128M
    mem_swappiness: 0
    command: redis-server --maxmemory 200mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # API Gateway - Entry point
  api-gateway:
    build:
      context: .
      dockerfile: apps/api-gateway/Dockerfile
    ports:
      - "3000:3000"
    env_file:
      - ./apps/api-gateway/.env
    environment:
      - USER_GRPC_URL=user-service:50051
      - DRIVER_GRPC_URL=driver-service:50052
      - TRIP_GRPC_URL=trip-service:50053
      - NODE_ENV=production
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - NODE_OPTIONS=--max-old-space-size=280 # ‚úÖ Bumped to 280 to prevent OOM during high traffic aggregation
      - ALLOW_GHOST_USERS=true # ‚úÖ Enable ghost users for load testing (bypasses Clerk API and database)
    depends_on:
      redis:
        condition: service_healthy
      user-service:
        condition: service_started
      driver-service:
        condition: service_started
      trip-service:
        condition: service_started
    networks:
      - uit-go-network
      - app-monitor-net # ‚úÖ Added for observability/k6 access
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: "0.5"
          memory: 350M # ‚úÖ Bumped to 350M (Safety buffer so Gateway doesn't die)
        reservations:
          cpus: "0.25"
          memory: 100M
    mem_swappiness: 0
    restart: unless-stopped

  # User Service
  user-service:
    build:
      context: .
      dockerfile: apps/user-service/Dockerfile
    environment:
      - USER_GRPC_URL=0.0.0.0:50051
      - DRIVER_GRPC_URL=driver-service:50052
      - TRIP_GRPC_URL=trip-service:50053
      - NODE_ENV=production
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - DATABASE_CONNECTION_LIMIT=2 # ‚ö†Ô∏è Critical for NeonDB free tier (2 * 2 replicas = 4 conn)
      - NODE_OPTIONS=--max-old-space-size=280 # ‚úÖ Bumped to 280 for safety buffer
    env_file:
      - ./apps/user-service/.env
    # ports:
    #   - "50051:50051"  # ‚ùå Disabled for scaling (avoids port conflicts)
    networks:
      - uit-go-network
    deploy:
      replicas: 2 # ‚úÖ Reduced from 3 - "Rule of Small"
      resources:
        limits:
          cpus: "0.6" # ‚úÖ High CPU to hammer Redis
          memory: 350M # ‚úÖ Bumped to 350M (Safety buffer)
        reservations:
          cpus: "0.25"
          memory: 100M
    mem_swappiness: 0
    restart: unless-stopped

  # Driver Service - Handles geospatial queries (HIGH CPU TO STRESS REDIS)
  driver-service:
    build:
      context: .
      dockerfile: apps/driver-service/Dockerfile
    environment:
      - DRIVER_GRPC_URL=0.0.0.0:50052
      - REDIS_URL=redis://redis:6379
      - MQTT_BROKER_URL=mqtt://mosquitto:1883
      - USER_GRPC_URL=user-service:50051
      - TRIP_GRPC_URL=trip-service:50053
      - NODE_ENV=production
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - DATABASE_CONNECTION_LIMIT=2 # ‚ö†Ô∏è Critical for NeonDB free tier (2 * 2 replicas = 4 conn)
      - NODE_OPTIONS=--max-old-space-size=280 # ‚úÖ Bumped to 280 for safety buffer
      # Pre-tuning Redis Geo bottleneck configuration (CLUSTER BOMB scenario)
      - USE_H3=true # false = pre-tuning (Redis Geo), true = post-tuning (H3)
      - MAX_DRIVER_SEARCH_COUNT=5000 # üí£ GREEDY QUERY: Fetch 5000 to maximize bottleneck
      - PREFER_REAL_DRIVERS=true # Prioritize real drivers over ghosts in results
      - H3_BATCH_SIZE=5 # üîß H3 batch size per bucket (fallback when dynamic sizing exceeds this)
    env_file:
      - ./apps/driver-service/.env
    # ports:
    #   - "50052:50052"  # ‚ùå Disabled for scaling (avoids port conflicts)
    depends_on:
      redis:
        condition: service_healthy
      mosquitto:
        condition: service_healthy
    networks:
      - uit-go-network
    deploy:
      replicas: 2 # ‚úÖ Reduced from 3 - "Rule of Small"
      resources:
        limits:
          cpus: "0.6" # ‚úÖ High CPU to hammer Redis bottleneck
          memory: 350M # ‚úÖ Bumped to 350M (Safety buffer)
        reservations:
          cpus: "0.25"
          memory: 150M
    mem_swappiness: 0
    restart: unless-stopped

  # Trip Service - Orchestrates trip lifecycle
  trip-service:
    build:
      context: .
      dockerfile: apps/trip-service/Dockerfile
    environment:
      - TRIP_GRPC_URL=0.0.0.0:50053
      - USER_GRPC_URL=user-service:50051
      - DRIVER_GRPC_URL=driver-service:50052
      - NODE_ENV=production
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - DATABASE_CONNECTION_LIMIT=2 # ‚ö†Ô∏è Critical for NeonDB free tier (2 * 2 replicas = 4 conn)
      - NODE_OPTIONS=--max-old-space-size=280 # ‚úÖ Bumped to 280 for safety buffer
    env_file:
      - ./apps/trip-service/.env
    # ports:
    #   - "50053:50053"  # ‚ùå Disabled for scaling (avoids port conflicts)
    networks:
      - uit-go-network
    deploy:
      replicas: 2 # ‚úÖ Reduced from 3 - "Rule of Small"
      resources:
        limits:
          cpus: "0.6" # ‚úÖ High CPU to hammer Redis
          memory: 350M # ‚úÖ Bumped to 350M (Safety buffer)
        reservations:
          cpus: "0.25"
          memory: 100M
    mem_swappiness: 0
    restart: unless-stopped

  # MQTT Broker - For driver location updates
  mosquitto:
    image: eclipse-mosquitto:2
    ports:
      - "1883:1883"
      - "9001:9001"
    volumes:
      - ./config/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf
    networks:
      - uit-go-network
      - app-monitor-net # ‚úÖ Added for mosquitto-exporter access
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: "0.5" # ‚úÖ Reduced since we aren't using it for load
          memory: 128M # ‚úÖ Optimized - MQTT not the bottleneck
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "mosquitto_sub", "-t", "$$SYS/#", "-C", "1", "-i", "healthcheck", "-W", "3"]
      interval: 30s
      timeout: 10s
      retries: 5

  # K6 Load Tester - Running INSIDE Docker network
  # This bypasses Windows Docker networking issues with MQTT connections
  k6-runner:
    build:
      context: .
      dockerfile: Dockerfile.k6
    networks:
      - uit-go-network # ‚úÖ Connects to Mosquitto and API Gateway directly (Linux-to-Linux)
      - app-monitor-net # ‚úÖ Connects to Prometheus directly
    volumes:
      - ./load-tests:/app/load-tests # Mount test scripts
    environment:
      # K6 Prometheus remote write
      - K6_PROMETHEUS_RW_SERVER_URL=http://prometheus:9090/api/v1/write
      - K6_PROMETHEUS_RW_TREND_AS_NATIVE_HISTOGRAM=true
      - K6_PROMETHEUS_RW_TREND_STATS=p(95),p(99),min,max
      # Signal that K6 is running inside Docker (used by config.js)
      - IN_DOCKER=true
    # Keep it stopped by default, we run it manually with 'docker compose run'
    profiles: ["test"]

networks:
  uit-go-network:
    driver: bridge
  app-monitor-net:
    external: true # ‚úÖ Shared network with observability stack

# ============================================================================
# OPTIMIZED RESOURCE ALLOCATION - "RULE OF SMALL" + SAFETY BUFFERS
# ============================================================================
# ‚ö†Ô∏è Optimized for 4GB RAM hosts with safety buffers for load testing
#
# Service          | CPU Limit | Memory Limit | Node Heap | Replicas | Total CPUs | Total Memory
# -----------------|-----------|--------------|-----------|----------|------------|-------------
# redis            | 0.5       | 256M         | N/A       | 1        | 0.5        | 256M
# api-gateway      | 0.5       | 256M         | 200M      | 1        | 0.5        | 256M
# user-service     | 0.6       | 256M         | 200M      | 2        | 1.2        | 512M
# driver-service   | 0.6       | 300M         | 240M      | 2        | 1.2        | 600M
# trip-service     | 0.6       | 256M         | 200M      | 2        | 1.2        | 512M
# mosquitto        | 1.0       | 256M         | N/A       | 1        | 1.0        | 256M
# -----------------|-----------|--------------|-----------|----------|------------|-------------
# TOTAL            | -         | -            | -         | 9        | 5.6 CPUs   | 2392M (~2.4GB)
#
# ‚úÖ NODE.JS MEMORY PROTECTION (CRITICAL FIX):
#    - NODE_OPTIONS=--max-old-space-size=XXX prevents OOMKilled (exit code 137)
#    - Heap size set to 75-80% of Docker memory limit
#    - Forces V8 garbage collection BEFORE Docker kills the container
#    - Without this, V8 sees 32GB host RAM and ignores Docker limits
#
# ‚úÖ BOTTLENECK ARCHITECTURE PRESERVED:
#    - Redis: 0.5 CPU (THE BOTTLENECK for driver matching algorithm)
#    - App Services: 4.6 CPUs attacking 0.5 CPU Redis = guaranteed bottleneck
#    - Total Memory: ~2.4GB (fits in 4GB RAM with ~1.6GB for host OS + observability stack)
#
# ‚úÖ SAFETY BUFFERS ADDED:
#    - All services have 20-30% headroom above typical usage
#    - Prevents OOMKilled during load testing spikes
#    - Node.js heaps sized to 80% of memory limits
#
# ‚úÖ DATABASE CONNECTION LIMITS:
#    - Each service: 2 connections max (DATABASE_CONNECTION_LIMIT=2)
#    - Total: 3 services √ó 2 replicas √ó 2 conn = 12 connections
#    - Fits comfortably within NeonDB Free Tier limit (~20-50 connections)
#
# ‚úÖ GHOST DRIVER STRATEGY:
#    - IDs starting with 'ghost:' bypass Clerk and NeonDB completely
#    - Ghost drivers only exist in Redis for geospatial bottleneck testing
#    - Allows 100k+ virtual drivers without hitting database limits
# ============================================================================

# ============================================================================
# HOW TO USE FOR ARCHITECTURAL COMPARISON
# ============================================================================
# SCENARIO 1: Pre-Improvements (Baseline)
#   - Use synchronous gRPC calls only
#   - No caching optimizations
#   - Basic error handling
#   - Run: docker-compose up --build -d
#   - Test: k6 run load-tests/baseline-test.js
#
# SCENARIO 2: Post-Improvements
#   - Async patterns (if implemented)
#   - Caching strategies
#   - Circuit breakers, retries
#   - Run: docker-compose up --build -d
#   - Test: k6 run load-tests/optimized-test.js
#
# COMPARISON:
#   - Same resources (2.5 CPUs, 4GB)
#   - Same replicas (1 per service)
#   - Only architectural differences matter
# ============================================================================
